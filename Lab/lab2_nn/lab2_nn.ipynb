{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS310 Natural Language Processing\n",
    "# Lab 2: Neural Text Classification\n",
    "\n",
    "This tutorial is adopted from the official PyTorch tutorial: *Text classification with the torchtext library*\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#text-classification-with-the-torchtext-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install torchtext\n",
    "\n",
    "Url: https://pypi.org/project/torchtext/\n",
    "```bash\n",
    "conda install -c pytorch torchtext\n",
    "```\n",
    "\n",
    "You may or may not need to manually install the following packages:\n",
    "    \n",
    "```bash\n",
    "pip install chardet\n",
    "pip install -U portalocker>=2.0.0\n",
    "```\n",
    "\n",
    "or with conda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge 'portalocker>=2.0.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import SST2 # SST2 is the sentiment analysis dataset, binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hide new secretions from the parental units', 0)\n",
      "('contains no wit , only labored gags', 0)\n",
      "('that loves its characters and communicates something rather beautiful about human nature', 1)\n",
      "('remains utterly satisfied to remain the same throughout', 0)\n",
      "('on the worst revenge-of-the-nerds clichés the filmmakers could dredge up', 0)\n",
      "(\"that 's far too tragic to merit such superficial treatment\", 0)\n",
      "('demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .', 1)\n",
      "('of saucy', 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the raw data\n",
    "train_iter = iter(SST2(split='train'))\n",
    "\n",
    "count = 0\n",
    "for item in train_iter:\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']\n",
      "['contains', 'no', 'wit', ',', 'only', 'labored', 'gags']\n",
      "['that', 'loves', 'its', 'characters', 'and', 'communicates', 'something', 'rather', 'beautiful', 'about', 'human', 'nature']\n",
      "['remains', 'utterly', 'satisfied', 'to', 'remain', 'the', 'same', 'throughout']\n",
      "['on', 'the', 'worst', 'revenge-of-the-nerds', 'clichés', 'the', 'filmmakers', 'could', 'dredge', 'up']\n",
      "['that', \"'\", 's', 'far', 'too', 'tragic', 'to', 'merit', 'such', 'superficial', 'treatment']\n",
      "['demonstrates', 'that', 'the', 'director', 'of', 'such', 'hollywood', 'blockbusters', 'as', 'patriot', 'games', 'can', 'still', 'turn', 'out', 'a', 'small', ',', 'personal', 'film', 'with', 'an', 'emotional', 'wallop', '.']\n",
      "['of', 'saucy']\n"
     ]
    }
   ],
   "source": [
    "# Check the output of yield_tokens()\n",
    "count = 0\n",
    "for tokens in yield_tokens(iter(SST2(split='train'))): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(iter(SST2(split='train'))), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 16, 1567]\n",
      "[4579, 92, 13266, 38, 1, 7742, 10000]\n",
      "[5, 7100]\n",
      "[224, 10, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "# Check the vocab\n",
    "print(vocab(['here', 'is', 'an', 'example']))\n",
    "print(vocab(['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units']))\n",
    "print(vocab(['of', 'saucy']))\n",
    "\n",
    "# What about unknown words, i.e., out-of-vocabulary (OOV) words?\n",
    "print(vocab(['here', 'is', 'a', '@#$@!#$%']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 10, 1, 16, 1567]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('here is the an example')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batch\n",
    "\n",
    "Define the `Collate_batch` function, which will be used to process the \"raw\" data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# mps for mac m1 chip\n",
    "print(torch.backends.mps.is_available()) \n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# The operator 'aten::_embedding_bag' is not currently implemented for the MPS device.\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0)) # 将每个样本的token数量添加到offsets列表中\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # 计算偏移量的累积和，从而得到每个批次数据在合并后的张量中的起始位置\n",
    "    token_ids = torch.cat(token_ids_list) # 得到一个包含所有样本的token IDs的tensor\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use collate_batch to generate the dataloader\n",
    "train_iter = SST2(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 label: tensor([0, 0, 1, 0, 0, 0, 1, 1], device='mps:0')\n",
      "batch 0 text: tensor([ 4579,    92, 13266,    38,     1,  7742, 10000,  2927,    58,   327,\n",
      "            2,    88,  1995,   548,    11,  1791,    18,    54,     4,  6088,\n",
      "           95,   184,   262,    36,   176,   624,   591,   679,  6403,     8,\n",
      "         2010,     1,   287,   701,    25,     1,   252,  5417,   551,     1,\n",
      "          357,   116,  4856,    53,    11,     7,     9,   171,    50,   780,\n",
      "            8,  1840,   120,   952,  1037,  2723,    11,     1,   107,     5,\n",
      "          120,   161,  3473,    14,  7011,  1444,    65,   149,   414,    49,\n",
      "            3,   394,     2,   529,    17,    15,    16,   205,  3149,     6,\n",
      "            5,  7100], device='mps:0')\n",
      "batch 0 offsets: tensor([ 0,  7, 14, 26, 34, 44, 55, 80], device='mps:0')\n",
      "Number of tokens:  82\n",
      "Number of examples in one batch:  8\n",
      "Example 1:  tensor([ 4579,    92, 13266,    38,     1,  7742, 10000], device='mps:0')\n",
      "Example 8:  tensor([   5, 7100], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 1: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 8: ', token_ids[offsets[7]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # EmbeddingBag处理不定长序列输入，它可以将输入的不定长序列转换为固定长度的词嵌入表示\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class) # full connection layer\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "train_iter = iter(SST2(split='train'))\n",
    "num_class = len(set([label for (_, label) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64 # embedding size\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_embedding_bag' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (labels, token_ids, offsets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 5\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# print(f\"batch {i} output: {output}\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[66], line 18\u001b[0m, in \u001b[0;36mTextClassificationModel.forward\u001b[0;34m(self, token_ids, offsets)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids, offsets):\n\u001b[0;32m---> 18\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(embedded)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/sparse.py:390\u001b[0m, in \u001b[0;36mEmbeddingBag.forward\u001b[0;34m(self, input, offsets, per_sample_weights)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, offsets: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, per_sample_weights: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of EmbeddingBag.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m          returned vectors filled by zeros.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_bag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mper_sample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_last_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/functional.py:2424\u001b[0m, in \u001b[0;36membedding_bag\u001b[0;34m(input, weight, offsets, max_norm, norm_type, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx)\u001b[0m\n\u001b[1;32m   2417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m per_sample_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   2419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_bag: per_sample_weights was not None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_sample_weights is only supported for mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m). Please open a feature request on GitHub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2422\u001b[0m     )\n\u001b[0;32m-> 2424\u001b[0m ret, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_bag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_sample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_last_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\n\u001b[1;32m   2426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_embedding_bag' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Functions\n",
    "Define train() and evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int): # criterion: loss function\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500 # print log every 500 batches\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # 裁剪，防止梯度爆炸。确保梯度的范数不超过给定的阈值（在这里是0.1\n",
    "        # 如果梯度的范数超过了阈值，那么梯度将按比例缩放，以使其范数不超过指定的阈值。防止梯度过大导致的参数更新过大而影响训练效果。\n",
    "        optimizer.step() # update the parameters using the gradients\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, label)\n",
    "            total_acc += (output.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters, loss, optimizer, and learning-rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 8  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # decay lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `criterion`, i.e., the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6672)\n",
      "loss non-reduced: tensor([0.7644, 0.5762, 0.6534, 0.8112, 0.6858, 0.6478, 0.7269, 0.4720])\n",
      "mean of loss non-reduced: tensor(0.6672)\n",
      "loss manually computed: tensor(0.7644)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# keep multiple losses for all samples in a batch, not just the mean\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none') \n",
    "\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = SST2(split=\"train\")\n",
    "test_iter = SST2(split=\"test\")\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7998 batches | accuracy    0.562\n",
      "| epoch   1 |  1000/ 7998 batches | accuracy    0.645\n",
      "| epoch   1 |  1500/ 7998 batches | accuracy    0.713\n",
      "| epoch   1 |  2000/ 7998 batches | accuracy    0.737\n",
      "| epoch   1 |  2500/ 7998 batches | accuracy    0.761\n",
      "| epoch   1 |  3000/ 7998 batches | accuracy    0.778\n",
      "| epoch   1 |  3500/ 7998 batches | accuracy    0.807\n",
      "| epoch   1 |  4000/ 7998 batches | accuracy    0.803\n",
      "| epoch   1 |  4500/ 7998 batches | accuracy    0.811\n",
      "| epoch   1 |  5000/ 7998 batches | accuracy    0.823\n",
      "| epoch   1 |  5500/ 7998 batches | accuracy    0.820\n",
      "| epoch   1 |  6000/ 7998 batches | accuracy    0.836\n",
      "| epoch   1 |  6500/ 7998 batches | accuracy    0.840\n",
      "| epoch   1 |  7000/ 7998 batches | accuracy    0.846\n",
      "| epoch   1 |  7500/ 7998 batches | accuracy    0.838\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 11.76s | valid accuracy    0.848 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 7998 batches | accuracy    0.881\n",
      "| epoch   2 |  1000/ 7998 batches | accuracy    0.880\n",
      "| epoch   2 |  1500/ 7998 batches | accuracy    0.876\n",
      "| epoch   2 |  2000/ 7998 batches | accuracy    0.881\n",
      "| epoch   2 |  2500/ 7998 batches | accuracy    0.872\n",
      "| epoch   2 |  3000/ 7998 batches | accuracy    0.880\n",
      "| epoch   2 |  3500/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  4000/ 7998 batches | accuracy    0.882\n",
      "| epoch   2 |  4500/ 7998 batches | accuracy    0.875\n",
      "| epoch   2 |  5000/ 7998 batches | accuracy    0.878\n",
      "| epoch   2 |  5500/ 7998 batches | accuracy    0.883\n",
      "| epoch   2 |  6000/ 7998 batches | accuracy    0.885\n",
      "| epoch   2 |  6500/ 7998 batches | accuracy    0.884\n",
      "| epoch   2 |  7000/ 7998 batches | accuracy    0.892\n",
      "| epoch   2 |  7500/ 7998 batches | accuracy    0.879\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 11.75s | valid accuracy    0.883 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 7998 batches | accuracy    0.909\n",
      "| epoch   3 |  1000/ 7998 batches | accuracy    0.908\n",
      "| epoch   3 |  1500/ 7998 batches | accuracy    0.907\n",
      "| epoch   3 |  2000/ 7998 batches | accuracy    0.901\n",
      "| epoch   3 |  2500/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  3000/ 7998 batches | accuracy    0.904\n",
      "| epoch   3 |  3500/ 7998 batches | accuracy    0.895\n",
      "| epoch   3 |  4000/ 7998 batches | accuracy    0.898\n",
      "| epoch   3 |  4500/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  5000/ 7998 batches | accuracy    0.906\n",
      "| epoch   3 |  5500/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  6000/ 7998 batches | accuracy    0.905\n",
      "| epoch   3 |  6500/ 7998 batches | accuracy    0.903\n",
      "| epoch   3 |  7000/ 7998 batches | accuracy    0.907\n",
      "| epoch   3 |  7500/ 7998 batches | accuracy    0.908\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 11.94s | valid accuracy    0.884 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 7998 batches | accuracy    0.923\n",
      "| epoch   4 |  1000/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  1500/ 7998 batches | accuracy    0.916\n",
      "| epoch   4 |  2000/ 7998 batches | accuracy    0.914\n",
      "| epoch   4 |  2500/ 7998 batches | accuracy    0.912\n",
      "| epoch   4 |  3000/ 7998 batches | accuracy    0.919\n",
      "| epoch   4 |  3500/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  4000/ 7998 batches | accuracy    0.916\n",
      "| epoch   4 |  4500/ 7998 batches | accuracy    0.920\n",
      "| epoch   4 |  5000/ 7998 batches | accuracy    0.913\n",
      "| epoch   4 |  5500/ 7998 batches | accuracy    0.919\n",
      "| epoch   4 |  6000/ 7998 batches | accuracy    0.910\n",
      "| epoch   4 |  6500/ 7998 batches | accuracy    0.905\n",
      "| epoch   4 |  7000/ 7998 batches | accuracy    0.911\n",
      "| epoch   4 |  7500/ 7998 batches | accuracy    0.898\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 11.44s | valid accuracy    0.894 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 7998 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 7998 batches | accuracy    0.918\n",
      "| epoch   5 |  1500/ 7998 batches | accuracy    0.925\n",
      "| epoch   5 |  2000/ 7998 batches | accuracy    0.923\n",
      "| epoch   5 |  2500/ 7998 batches | accuracy    0.925\n",
      "| epoch   5 |  3000/ 7998 batches | accuracy    0.932\n",
      "| epoch   5 |  3500/ 7998 batches | accuracy    0.924\n",
      "| epoch   5 |  4000/ 7998 batches | accuracy    0.917\n",
      "| epoch   5 |  4500/ 7998 batches | accuracy    0.924\n",
      "| epoch   5 |  5000/ 7998 batches | accuracy    0.919\n",
      "| epoch   5 |  5500/ 7998 batches | accuracy    0.920\n",
      "| epoch   5 |  6000/ 7998 batches | accuracy    0.915\n",
      "| epoch   5 |  6500/ 7998 batches | accuracy    0.918\n",
      "| epoch   5 |  7000/ 7998 batches | accuracy    0.917\n",
      "| epoch   5 |  7500/ 7998 batches | accuracy    0.923\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 11.69s | valid accuracy    0.876 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 7998 batches | accuracy    0.934\n",
      "| epoch   6 |  1000/ 7998 batches | accuracy    0.948\n",
      "| epoch   6 |  1500/ 7998 batches | accuracy    0.941\n",
      "| epoch   6 |  2000/ 7998 batches | accuracy    0.946\n",
      "| epoch   6 |  2500/ 7998 batches | accuracy    0.933\n",
      "| epoch   6 |  3000/ 7998 batches | accuracy    0.944\n",
      "| epoch   6 |  3500/ 7998 batches | accuracy    0.948\n",
      "| epoch   6 |  4000/ 7998 batches | accuracy    0.941\n",
      "| epoch   6 |  4500/ 7998 batches | accuracy    0.945\n",
      "| epoch   6 |  5000/ 7998 batches | accuracy    0.946\n",
      "| epoch   6 |  5500/ 7998 batches | accuracy    0.947\n",
      "| epoch   6 |  6000/ 7998 batches | accuracy    0.946\n",
      "| epoch   6 |  6500/ 7998 batches | accuracy    0.945\n",
      "| epoch   6 |  7000/ 7998 batches | accuracy    0.950\n",
      "| epoch   6 |  7500/ 7998 batches | accuracy    0.948\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 11.82s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 7998 batches | accuracy    0.951\n",
      "| epoch   7 |  1000/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  1500/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  2000/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  2500/ 7998 batches | accuracy    0.952\n",
      "| epoch   7 |  3000/ 7998 batches | accuracy    0.947\n",
      "| epoch   7 |  3500/ 7998 batches | accuracy    0.943\n",
      "| epoch   7 |  4000/ 7998 batches | accuracy    0.953\n",
      "| epoch   7 |  4500/ 7998 batches | accuracy    0.943\n",
      "| epoch   7 |  5000/ 7998 batches | accuracy    0.947\n",
      "| epoch   7 |  5500/ 7998 batches | accuracy    0.945\n",
      "| epoch   7 |  6000/ 7998 batches | accuracy    0.948\n",
      "| epoch   7 |  6500/ 7998 batches | accuracy    0.949\n",
      "| epoch   7 |  7000/ 7998 batches | accuracy    0.950\n",
      "| epoch   7 |  7500/ 7998 batches | accuracy    0.952\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 11.74s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 7998 batches | accuracy    0.951\n",
      "| epoch   8 |  1000/ 7998 batches | accuracy    0.948\n",
      "| epoch   8 |  1500/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  2000/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  2500/ 7998 batches | accuracy    0.950\n",
      "| epoch   8 |  3000/ 7998 batches | accuracy    0.956\n",
      "| epoch   8 |  3500/ 7998 batches | accuracy    0.953\n",
      "| epoch   8 |  4000/ 7998 batches | accuracy    0.948\n",
      "| epoch   8 |  4500/ 7998 batches | accuracy    0.952\n",
      "| epoch   8 |  5000/ 7998 batches | accuracy    0.948\n",
      "| epoch   8 |  5500/ 7998 batches | accuracy    0.948\n",
      "| epoch   8 |  6000/ 7998 batches | accuracy    0.946\n",
      "| epoch   8 |  6500/ 7998 batches | accuracy    0.947\n",
      "| epoch   8 |  7000/ 7998 batches | accuracy    0.949\n",
      "| epoch   8 |  7500/ 7998 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 11.72s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 7998 batches | accuracy    0.959\n",
      "| epoch   9 |  1000/ 7998 batches | accuracy    0.945\n",
      "| epoch   9 |  1500/ 7998 batches | accuracy    0.947\n",
      "| epoch   9 |  2000/ 7998 batches | accuracy    0.955\n",
      "| epoch   9 |  2500/ 7998 batches | accuracy    0.954\n",
      "| epoch   9 |  3000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  3500/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  4000/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  4500/ 7998 batches | accuracy    0.949\n",
      "| epoch   9 |  5000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  5500/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  6000/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  6500/ 7998 batches | accuracy    0.953\n",
      "| epoch   9 |  7000/ 7998 batches | accuracy    0.948\n",
      "| epoch   9 |  7500/ 7998 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 11.60s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 7998 batches | accuracy    0.950\n",
      "| epoch  10 |  1000/ 7998 batches | accuracy    0.954\n",
      "| epoch  10 |  1500/ 7998 batches | accuracy    0.954\n",
      "| epoch  10 |  2000/ 7998 batches | accuracy    0.956\n",
      "| epoch  10 |  2500/ 7998 batches | accuracy    0.955\n",
      "| epoch  10 |  3000/ 7998 batches | accuracy    0.956\n",
      "| epoch  10 |  3500/ 7998 batches | accuracy    0.947\n",
      "| epoch  10 |  4000/ 7998 batches | accuracy    0.957\n",
      "| epoch  10 |  4500/ 7998 batches | accuracy    0.956\n",
      "| epoch  10 |  5000/ 7998 batches | accuracy    0.945\n",
      "| epoch  10 |  5500/ 7998 batches | accuracy    0.958\n",
      "| epoch  10 |  6000/ 7998 batches | accuracy    0.949\n",
      "| epoch  10 |  6500/ 7998 batches | accuracy    0.951\n",
      "| epoch  10 |  7000/ 7998 batches | accuracy    0.954\n",
      "| epoch  10 |  7500/ 7998 batches | accuracy    0.952\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 11.55s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with Test Data\n",
    "\n",
    "This is a necessary step. But since the `test` split of SST2 is not annotated, we will use the `dev` split here to pretend it is the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.912\n"
     ]
    }
   ],
   "source": [
    "accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Test the model with a few unannotated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The plot was \" is a negative sentiment.\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = ['negative', 'positive']\n",
    "\n",
    "def predict(text, model, vocab, tokenizer, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(vocab(tokenizer(text)), device=device)\n",
    "        output = model(text, torch.tensor([0], device=device))\n",
    "        return labels[output.argmax(1).item()]\n",
    "\n",
    "ex_text_str = \"The plot was \"\n",
    "\n",
    "print(f\"\\\"{ex_text_str}\\\" is a %s sentiment.\" % (predict(ex_text_str, model, vocab, tokenizer, sentiment_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
