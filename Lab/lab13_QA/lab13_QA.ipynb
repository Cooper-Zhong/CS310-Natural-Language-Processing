{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 13: Explore Question-Answering Models and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will practice with running pretrained models on question-answering tasks. The we demonstrate with is `distilbert-base-uncased`, which is a smaller version of BERT.\n",
    "\n",
    "We will use the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) datast provided in the [Datasets](https://github.com/huggingface/datasets) library. Make sure to install the library:\n",
    "\n",
    "```bash\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Explore the SQuAD dataset\n",
    "\n",
    "First, let's load the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "squad_dataset = load_dataset('./squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `squad_dataset` object is a `DefaultDict` that contains keys for the train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a data instance, you can specify the split and index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that teh answer is indicated by its span start index (at character `515`) in the passage text. \n",
    "\n",
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572f8123a23a5019007fc6ad</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>The jurisdictions of the city's administrative agencies are, in ascending order of size: the Hyderabad Police area, Hyderabad district, the GHMC area (\"Hyderabad city\") and the area under the Hyderabad Metropolitan Development Authority (HMDA). The HMDA is an apolitical urban planning agency that covers the GHMC and its suburbs, extending to 54 mandals in five districts encircling the city. It coordinates the development activities of GHMC and suburban municipalities and manages the administration of bodies such as the Hyderabad Metropolitan Water Supply and Sewerage Board (HMWSSB).</td>\n",
       "      <td>Which Hyderabad agency is responsible for the largest area?</td>\n",
       "      <td>{'text': ['Hyderabad Police'], 'answer_start': [93]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5706b0a62eaba6190074ac3b</td>\n",
       "      <td>House_music</td>\n",
       "      <td>The house scene in cities such as Birmingham, Leeds, Sheffield and London were also provided with many underground Pirate Radio stations and DJs alike which helped bolster an already contagious, but otherwise ignored by the mainstream, music genre. The earliest and influential UK house and techno record labels such as Warp Records and Network Records (otherwise known as Kool Kat records) helped introduce American and later Italian dance music to Britain as well as promoting select UK dance music acts.</td>\n",
       "      <td>what helped to bolster house music in the uk?</td>\n",
       "      <td>{'text': ['underground Pirate Radio stations and DJs'], 'answer_start': [103]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57109ebeb654c5140001f9db</td>\n",
       "      <td>Age_of_Enlightenment</td>\n",
       "      <td>The most influential publication of the Enlightenment was the Encyclop√©die, compiled by Denis Diderot and (until 1759) by Jean le Rond d'Alembert and a team of 150 scientists and philosophers. It was published between 1751 and 1772 in thirty-five volumes, and spread the ideas of the Enlightenment across Europe and beyond. Other landmark publications were the Dictionnaire philosophique (Philosophical Dictionary, 1764) and Letters on the English (1733) written by Voltaire; Rousseau's Discourse on Inequality (1754) and The Social Contract (1762); and Montesquieu's Spirit of the Laws (1748). The ideas of the Enlightenment played a major role in inspiring the French Revolution, which began in 1789. After the Revolution, the Enlightenment was followed by an opposing intellectual movement known as Romanticism.</td>\n",
       "      <td>What year did the French Revolution begin?</td>\n",
       "      <td>{'text': ['1789'], 'answer_start': [697]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(squad_dataset[\"train\"], num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Preprocess the data\n",
    "\n",
    "Before we feed the data to a model for fine-tuning, there is some preprocessing needed: \n",
    "- Tokenize the input text\n",
    "- Put it in the format expected by the model\n",
    "- Generate other inputs the model requires\n",
    "\n",
    "To do all of this, we need to instantiate a tokenizer that is compatible with the model we want to use, i.e., `distilbert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"./distilbert-base-uncased\" # If loaded locally, make sure you have the model downloaded first\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly call this tokenizer on two sentences (e.g., question and context):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 102, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Architecturally, the school has a Catholic character.', 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step in QA is to deal with very **long documents**. If longer than the maximum input size of model, then removing part of context might result in losing the answer.\n",
    "\n",
    "To handle this, we will allow a long document to give several input *features*, each of length shorter than the maximum size. \n",
    "\n",
    "Also, in case the answer is split between two features, we allow some overlap between features, controlled by `doc_stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine on one long example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(squad_dataset[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = squad_dataset[\"train\"][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without truncation, its length is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example['question'], example['context'])['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we truncate, the resulting length is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we never want to truncate the question, so we specify `truncation='only_second`. \n",
    "\n",
    "Now, we further tell the tokenizer to return the overlaping features, by setting `return_overflowing_tokens=True` and `stride=doc_stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 157]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "\n",
    "print([len(x) for x in tokenized_example[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the two features decoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[CLS] how many wins does the notre dame men's basketball team have? [SEP] \"\n",
      " \"the men's basketball team has over 1, 600 wins, one of only 12 schools who \"\n",
      " 'have reached that mark, and have appeared in 28 ncaa tournaments. former '\n",
      " 'player austin carr holds the record for most points scored in a single game '\n",
      " 'of the tournament with 61. although the team has never won the ncaa '\n",
      " 'tournament, they were named by the helms athletic foundation as national '\n",
      " 'champions twice. the team has orchestrated a number of upsets of number one '\n",
      " \"ranked teams, the most notable of which was ending ucla's record 88 - game \"\n",
      " 'winning streak in 1974. the team has beaten an additional eight number - one '\n",
      " \"teams, and those nine wins rank second, to ucla's 10, all - time in wins \"\n",
      " 'against the top team. the team plays in newly renovated purcell pavilion ( '\n",
      " 'within the edmund p. joyce center ), which reopened for the beginning of the '\n",
      " '2009 ‚Äì 2010 season. the team is coached by mike brey, who, as of the 2014 ‚Äì '\n",
      " '15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in '\n",
      " '2009 they were invited to the nit, where they advanced to the semifinals but '\n",
      " 'were beaten by penn state who went on and beat baylor in the championship. '\n",
      " 'the 2010 ‚Äì 11 team concluded its regular season ranked number seven in the '\n",
      " \"country, with a record of 25 ‚Äì 5, brey's fifth straight 20 - win season, and \"\n",
      " 'a second - place finish in the big east. during the 2014 - 15 season, the '\n",
      " 'team went 32 - 6 and won the acc conference tournament, later advancing to '\n",
      " 'the elite 8, where the fighting irish lost on a missed buzzer - beater '\n",
      " 'against then undefeated kentucky. led by nba draft picks jerian grant and '\n",
      " 'pat connaughton, the fighting irish beat the eventual national champion duke '\n",
      " 'blue devils twice during the season. the 32 wins were [SEP]')\n",
      "(\"[CLS] how many wins does the notre dame men's basketball team have? [SEP] \"\n",
      " 'championship. the 2010 ‚Äì 11 team concluded its regular season ranked number '\n",
      " \"seven in the country, with a record of 25 ‚Äì 5, brey's fifth straight 20 - \"\n",
      " 'win season, and a second - place finish in the big east. during the 2014 - '\n",
      " '15 season, the team went 32 - 6 and won the acc conference tournament, later '\n",
      " 'advancing to the elite 8, where the fighting irish lost on a missed buzzer - '\n",
      " 'beater against then undefeated kentucky. led by nba draft picks jerian grant '\n",
      " 'and pat connaughton, the fighting irish beat the eventual national champion '\n",
      " 'duke blue devils twice during the season. the 32 wins were the most by the '\n",
      " 'fighting irish team since 1908 - 09. [SEP]')\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    pprint(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we nned to find out in which of the two features the answer is, and where exactly it starts and ends.\n",
    "\n",
    "Thankfully, the tokenizer can help us by returning the `offset_mapping` that gives the start and end character of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "print(offsets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, the very first token (`[CLS]`) has `(0, 0)` because it doesn't correspond to any part of the question/answer.\n",
    "\n",
    "The second token corresponds to the span from character 0 to 3 in the context, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how\n",
      "How\n"
     ]
    }
   ],
   "source": [
    "token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens(token_id))\n",
    "\n",
    "token_offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(example[\"question\"][token_offsets[0]:token_offsets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on to the next step, we just have to distinguish between the offsets for `question` and those for `context`. The `sequence_ids` method can be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sequence_ids): 384\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "\n",
    "print('len(sequence_ids):', len(sequence_ids))\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns None for the special tokens; then `0` for tokens from the first sequence (i.e., the `question`), and `1` for tokens from the second sequence (i.e., the `context`).\n",
    "\n",
    "It tells us that we need to find the span of answer among all `1` tokens.\n",
    "\n",
    "Now, we are ready to use `offset_mapping` to find the position of the start and end tokens of the `answer` in a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['over 1,600'], 'answer_start': [30]}\n",
      "ans_start: 30\n",
      "end_char: 40\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "ans_start = answers[\"answer_start\"][0]\n",
    "ans_end = ans_start + len(answers[\"text\"][0])\n",
    "\n",
    "print(answers)\n",
    "print('ans_start:', ans_start)\n",
    "print('end_char:', ans_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `token_start_index` and `token_end_index` be the initial search range for the answer span, initialize them properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_start_index: 16\n",
      "offsets[token_start_index]: (0, 3)\n"
     ]
    }
   ],
   "source": [
    "# Find the position of the first `1` token\n",
    "### START YOUR CODE ###\n",
    "token_start_index = 0\n",
    "for i, (sequence_id) in enumerate(sequence_ids):\n",
    "    if sequence_id == 1:\n",
    "        token_start_index = i\n",
    "        break\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('token_start_index:', token_start_index)\n",
    "print('offsets[token_start_index]:', offsets[token_start_index])\n",
    "# Expected output\n",
    "# token_start_index: 16\n",
    "# offsets[token_start_index]: (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_end_index: 382\n",
      "offsets[token_end_index]: (1665, 1669)\n"
     ]
    }
   ],
   "source": [
    "# Find the position of the last `1` token\n",
    "### START YOUR CODE ###\n",
    "token_end_index = None\n",
    "for i, (sequence_id) in reversed(list(enumerate(sequence_ids))):\n",
    "    if sequence_id == 1:\n",
    "        token_end_index = i\n",
    "        break\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('token_end_index:', token_end_index)\n",
    "print('offsets[token_end_index]:', offsets[token_end_index])\n",
    "# Expected output\n",
    "# token_end_index: 382\n",
    "# offsets[token_end_index]: (1665, 1669)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, detect if `ans_start` and `ans_end` is within the initial search range. \n",
    "\n",
    "If they do, then find the start and end indices of tokens, whose offsets encompass `ans_start` and `ans_end`, repectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "token_ids = tokenized_example['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 26\n",
      "(30, 34) (37, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_start_index = 16\n",
    "token_end_index = 382 # reset\n",
    "\n",
    "# Detect if the answer is within the initial search range\n",
    "### START YOUR CODE ###\n",
    "if token_start_index > ans_start or ans_end > token_end_index: \n",
    "    # Change `None` to your condition\n",
    "    print('The answer is not in this feature.')\n",
    "### END YOUR CODE ###\n",
    "else:\n",
    "    # Find the start and end indices of the tokens, whose offsets encompass the ans_start and ans_end\n",
    "    ### START YOUR CODE ###\n",
    "    for i in range(token_start_index, token_end_index):\n",
    "        if offsets[i][0] <= ans_start:\n",
    "            start_position = i\n",
    "        if offsets[i][1] >= ans_end:\n",
    "            end_position = i\n",
    "            break\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(start_position, end_position)\n",
    "print(offsets[start_position], offsets[end_position])\n",
    "\n",
    "# Expected output\n",
    "# 23 26\n",
    "# (30,34) (37,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check that it is indeed the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
